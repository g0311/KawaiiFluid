// Copyright KawaiiFluid Team. All Rights Reserved.
// GPU Fluid Physics - Radix Sort for Morton Code Sorting
//
// Implements parallel radix sort on GPU for sorting particles by Morton code.
// Uses 8-bit radix (256 buckets) with 3 passes for 21-bit Morton codes (24-bit coverage).
//
// Pipeline:
// 1. Histogram: Each thread group computes per-warp histogram
// 2. Global Prefix Sum: Compute global offsets for each bucket
// 3. Scatter: Write elements to final positions (STABLE via Wave Intrinsics ranking)
//
// OPTIMIZATIONS:
// - 8-bit radix reduces passes from 6 to 3, halving dispatch overhead
// - Wave Intrinsics for O(1) intra-warp ranking (replaces O(128) LDS scan)
// - Wave-level histogram reduces LDS atomic contention
//
// NOTE: Wave Intrinsics are SAFE here because:
// - All threads execute same number of iterations ([unroll] loops)
// - No early returns - all threads complete together
// - No divergent control flow within warp
//
// LDS Usage: ~9KB (Warp histogram 8KB + Base offsets 1KB)

#include "/Engine/Public/Platform.ush"
#include "/Engine/Private/Common.ush"

//=============================================================================
// Wave Intrinsics Support Detection
// SM6.0+ required for Wave Intrinsics (WaveActiveBallot, WaveGetLaneIndex, etc.)
// If platform doesn't define this macro, assume NOT supported (safe fallback)
//=============================================================================
#ifndef COMPILER_SUPPORTS_WAVE_INTRINSICS
    #define COMPILER_SUPPORTS_WAVE_INTRINSICS 0   // Default: LDS fallback (safe for all platforms)
#endif

//=============================================================================
// Configuration - 8-bit Radix (256 buckets)
//=============================================================================

#define RADIX_BITS 8                    // 8 bits per pass (256 buckets)
#define RADIX_SIZE (1 << RADIX_BITS)    // 256 buckets
#define RADIX_MASK (RADIX_SIZE - 1)     // 0xFF

#define THREAD_GROUP_SIZE 256
#define ELEMENTS_PER_THREAD 4           // Each thread processes 4 elements
#define ELEMENTS_PER_GROUP (THREAD_GROUP_SIZE * ELEMENTS_PER_THREAD)  // 1024

#define WARP_SIZE 32
#define NUM_WARPS (THREAD_GROUP_SIZE / WARP_SIZE)  // 8 warps

//=============================================================================
// Shader Parameters
//=============================================================================

// Input buffers (read-only, bound as SRV)
StructuredBuffer<uint> KeysIn;
StructuredBuffer<uint> ValuesIn;

// Output buffers (write, bound as UAV)
RWStructuredBuffer<uint> KeysOut;
RWStructuredBuffer<uint> ValuesOut;

// Histogram and prefix sum buffers
// Note: These have both RW and RO versions for different shader passes
RWStructuredBuffer<uint> Histogram;      // [NumGroups * RADIX_SIZE] - UAV for Histogram/PrefixSum passes
RWStructuredBuffer<uint> GlobalOffsets;  // [RADIX_SIZE] - UAV for PrefixSum passes

// Read-only versions for Scatter pass (bound as SRV)
StructuredBuffer<uint> HistogramSRV;
StructuredBuffer<uint> GlobalOffsetsSRV;

int ElementCount;
int BitOffset;       // Current bit position (0, 8, 16 for 8-bit radix)
int NumGroups;       // Number of thread groups

//=============================================================================
// Shared Memory for Histogram Pass
//=============================================================================

groupshared uint LocalHistogram[RADIX_SIZE];                 // 1KB - per-group histogram

//=============================================================================
// Pass 1: Histogram Computation (8-bit radix)
// Each thread group counts elements per bucket using warp-level atomics
//=============================================================================

[numthreads(THREAD_GROUP_SIZE, 1, 1)]
void RadixSortHistogramCS(
    uint3 GroupId : SV_GroupID,
    uint3 GroupThreadId : SV_GroupThreadID,
    uint3 DispatchThreadId : SV_DispatchThreadID)
{
    uint localIdx = GroupThreadId.x;
    uint groupIdx = GroupId.x;
    uint globalBase = groupIdx * ELEMENTS_PER_GROUP;

    // Initialize local histogram (256 threads clear 256 buckets - perfect 1:1)
    LocalHistogram[localIdx] = 0;
    GroupMemoryBarrierWithGroupSync();

    // Load elements and count (only valid elements)
    [unroll]
    for (uint i = 0; i < ELEMENTS_PER_THREAD; ++i)
    {
        uint elementIdx = globalBase + localIdx * ELEMENTS_PER_THREAD + i;

        if (elementIdx < (uint)ElementCount)
        {
            uint key = KeysIn[elementIdx];
            uint digit = (key >> BitOffset) & RADIX_MASK;
            InterlockedAdd(LocalHistogram[digit], 1);
        }
    }
    GroupMemoryBarrierWithGroupSync();

    // Write histogram to global memory (256 threads write 256 values - coalesced)
    Histogram[groupIdx * RADIX_SIZE + localIdx] = LocalHistogram[localIdx];
}

//=============================================================================
// Pass 2: Global Prefix Sum over Histograms
// Computes global offsets for each bucket across all groups
// 256 threads process 256 buckets in parallel
//=============================================================================

[numthreads(RADIX_SIZE, 1, 1)]
void RadixSortGlobalPrefixSumCS(uint3 DispatchThreadId : SV_DispatchThreadID)
{
    uint bucketIdx = DispatchThreadId.x;
    if (bucketIdx >= RADIX_SIZE)
    {
        return;
    }

    // Sum counts for this bucket across all groups
    uint totalCount = 0;
    for (int g = 0; g < NumGroups; ++g)
    {
        uint count = Histogram[g * RADIX_SIZE + bucketIdx];
        Histogram[g * RADIX_SIZE + bucketIdx] = totalCount;  // Store prefix for this group
        totalCount += count;
    }

    // Store total count in global offsets (will be prefix-summed next)
    GlobalOffsets[bucketIdx] = totalCount;
}

//=============================================================================
// Pass 2b: Prefix Sum over Global Offsets
// Converts bucket totals to bucket start positions
//
// Wave-optimized: O(log n) parallel prefix sum using WavePrefixSum
// Fallback: O(n) sequential prefix sum
//
// Both versions dispatch with (1, 1, 1) groups - thread count differs in numthreads
//=============================================================================

#if COMPILER_SUPPORTS_WAVE_INTRINSICS

// Wave-optimized version: 256 threads with WavePrefixSum + LDS cross-wave
// Complexity: O(log waveSize) intra-wave + O(numWaves) cross-wave
groupshared uint BucketWaveTotals[8];  // Max 8 waves (256 threads / 32 wave size)

[numthreads(RADIX_SIZE, 1, 1)]
void RadixSortBucketPrefixSumCS(uint3 GroupThreadId : SV_GroupThreadID)
{
    uint idx = GroupThreadId.x;
    uint laneIdx = WaveGetLaneIndex();
    uint waveSize = WaveGetLaneCount();
    uint waveIdx = idx / waveSize;

    // Step 1: Read bucket count
    uint myCount = GlobalOffsets[idx];

    // Step 2: Intra-wave exclusive prefix sum (O(log waveSize) internally)
    uint wavePrefixSum = WavePrefixSum(myCount);
    uint waveTotal = WaveActiveSum(myCount);

    // Step 3: First lane of each wave stores wave total to LDS
    if (WaveIsFirstLane())
    {
        BucketWaveTotals[waveIdx] = waveTotal;
    }
    GroupMemoryBarrierWithGroupSync();

    // Step 4: Compute cross-wave prefix (small sequential loop, max 8 iterations)
    uint crossWavePrefix = 0;
    for (uint w = 0; w < waveIdx; ++w)
    {
        crossWavePrefix += BucketWaveTotals[w];
    }

    // Step 5: Final prefix = cross-wave prefix + intra-wave prefix
    GlobalOffsets[idx] = crossWavePrefix + wavePrefixSum;
}

#else

// Fallback: Single-threaded sequential prefix sum
// Complexity: O(n) where n = RADIX_SIZE (256)
[numthreads(1, 1, 1)]
void RadixSortBucketPrefixSumCS(uint3 DispatchThreadId : SV_DispatchThreadID)
{
    uint prefixSum = 0;
    for (uint i = 0; i < RADIX_SIZE; ++i)
    {
        uint count = GlobalOffsets[i];
        GlobalOffsets[i] = prefixSum;
        prefixSum += count;
    }
}

#endif

//=============================================================================
// Pass 3: Scatter Elements to Final Positions (STABLE SORT)
//
// 8-bit Radix with Warp-level Histogram + LDS-based Stable Ranking
//
// Algorithm:
// 1. Load elements and store digits to LDS for stable ranking
// 2. Build per-warp histogram using atomics (just counting)
// 3. Compute per-warp prefix sums (deterministic)
// 4. Scatter with LDS-based intra-warp ranking (STABLE)
//
// LDS Usage:
// - WarpBucketData[8 * 256] = 8KB (warp histogram + prefix sums)
// - DigitBaseOffsets[256] = 1KB (global + group offsets)
// - ElementDigits[1024] = 4KB (for stable intra-warp ranking)
// Total: 13KB (fits in 32KB LDS limit)
//=============================================================================

groupshared uint WarpBucketData[NUM_WARPS * RADIX_SIZE];  // 8KB
groupshared uint DigitBaseOffsets[RADIX_SIZE];            // 1KB
groupshared uint ElementDigits[ELEMENTS_PER_GROUP];       // 4KB

[numthreads(THREAD_GROUP_SIZE, 1, 1)]
void RadixSortScatterCS(
    uint3 GroupId : SV_GroupID,
    uint3 GroupThreadId : SV_GroupThreadID)
{
    uint localIdx = GroupThreadId.x;
    uint groupIdx = GroupId.x;
    uint globalBase = groupIdx * ELEMENTS_PER_GROUP;
    uint warpId = localIdx / WARP_SIZE;

    //=========================================================================
    // Step 1: Load elements and store digits to LDS
    //=========================================================================

    uint myKeys[ELEMENTS_PER_THREAD];
    uint myValues[ELEMENTS_PER_THREAD];
    uint myDigits[ELEMENTS_PER_THREAD];
    uint myValid[ELEMENTS_PER_THREAD];

    [unroll]
    for (uint i = 0; i < ELEMENTS_PER_THREAD; ++i)
    {
        uint elementIdx = globalBase + localIdx * ELEMENTS_PER_THREAD + i;
        uint localElementIdx = localIdx * ELEMENTS_PER_THREAD + i;

        if (elementIdx < (uint)ElementCount)
        {
            myKeys[i] = KeysIn[elementIdx];
            myValues[i] = ValuesIn[elementIdx];
            myDigits[i] = (myKeys[i] >> BitOffset) & RADIX_MASK;
            myValid[i] = 1;
            ElementDigits[localElementIdx] = myDigits[i];
        }
        else
        {
            myKeys[i] = 0xFFFFFFFF;
            myValues[i] = 0;
            myDigits[i] = 0xFF;
            myValid[i] = 0;
            ElementDigits[localElementIdx] = 0xFFFFFFFF;  // Invalid marker
        }
    }

    //=========================================================================
    // Step 2: Clear and build per-warp histograms
    //=========================================================================

    // Clear warp bucket data (256 threads clear 8*256=2048 values)
    for (uint b = localIdx; b < NUM_WARPS * RADIX_SIZE; b += THREAD_GROUP_SIZE)
    {
        WarpBucketData[b] = 0;
    }
    GroupMemoryBarrierWithGroupSync();

    // Count elements per warp per bucket (atomics for counting only)
    [unroll]
    for (uint i = 0; i < ELEMENTS_PER_THREAD; ++i)
    {
        if (myValid[i])
        {
            InterlockedAdd(WarpBucketData[warpId * RADIX_SIZE + myDigits[i]], 1);
        }
    }
    GroupMemoryBarrierWithGroupSync();

    //=========================================================================
    // Step 3: Load global offsets and compute per-warp prefix sums
    // 256 threads handle 256 buckets in parallel
    //=========================================================================

    {
        uint bucket = localIdx;
        DigitBaseOffsets[bucket] = GlobalOffsetsSRV[bucket] + HistogramSRV[groupIdx * RADIX_SIZE + bucket];

        // Prefix sum across warps for this bucket (8 iterations, sequential)
        uint sum = 0;
        for (uint w = 0; w < NUM_WARPS; ++w)
        {
            uint idx = w * RADIX_SIZE + bucket;
            uint count = WarpBucketData[idx];
            WarpBucketData[idx] = sum;  // Replace count with prefix
            sum += count;
        }
    }
    GroupMemoryBarrierWithGroupSync();

    //=========================================================================
    // Step 4: Scatter with STABLE intra-warp ranking
    //
    // For stability: rank = count of elements with same digit that come
    // BEFORE this element in deterministic order (thread 0 elem 0 first,
    // then thread 0 elem 1, ..., thread 31 elem 3).
    //
    // This scan is O(128) per element but guarantees deterministic ordering.
    //=========================================================================

    uint warpStartElement = warpId * (WARP_SIZE * ELEMENTS_PER_THREAD);  // 128 elements per warp

    [unroll]
    for (uint i = 0; i < ELEMENTS_PER_THREAD; ++i)
    {
        if (myValid[i])
        {
            uint digit = myDigits[i];
            uint myElementIdx = localIdx * ELEMENTS_PER_THREAD + i;
            uint myWarpLocalIdx = myElementIdx - warpStartElement;

            // Count elements with same digit before this one within the warp
            // This is the key to STABLE sort - deterministic ordering
            uint rankInWarp = 0;
            for (uint j = 0; j < myWarpLocalIdx; ++j)
            {
                if (ElementDigits[warpStartElement + j] == digit)
                {
                    rankInWarp++;
                }
            }

            // Final output position:
            // Global bucket start + warp's prefix within group + rank within warp
            uint outputPos = DigitBaseOffsets[digit]
                           + WarpBucketData[warpId * RADIX_SIZE + digit]
                           + rankInWarp;

            KeysOut[outputPos] = myKeys[i];
            ValuesOut[outputPos] = myValues[i];
        }
    }
}

//=============================================================================
// Simple Single-Pass Sort for Small Arrays (< 1024 elements)
// Uses shared memory for complete sort within one group
// Note: Uses counting sort with atomics - not stable but correct for Morton sorting
//
// Shared memory usage: ~20KB (well under 32KB limit)
//=============================================================================

#define SMALL_SORT_THREAD_COUNT 256
#define SMALL_SORT_ELEMENTS_PER_THREAD 4

groupshared uint SmallSortKeys[1024];      // 4KB
groupshared uint SmallSortValues[1024];    // 4KB
groupshared uint SmallSortTemp[1024];      // 4KB
groupshared uint SmallSortTempV[1024];     // 4KB
groupshared uint SmallHistogram[RADIX_SIZE];        // 1KB
groupshared uint SmallPrefixSum[RADIX_SIZE];        // 1KB
#if COMPILER_SUPPORTS_WAVE_INTRINSICS
groupshared uint SmallWaveTotals[8];       // 32B - wave totals for parallel prefix sum
#endif

[numthreads(SMALL_SORT_THREAD_COUNT, 1, 1)]
void RadixSortSmallCS(uint3 GroupThreadId : SV_GroupThreadID)
{
    uint localIdx = GroupThreadId.x;
    uint elemIdx, loopIdx;

    // Load data to shared memory
    [unroll]
    for (loopIdx = 0; loopIdx < SMALL_SORT_ELEMENTS_PER_THREAD; ++loopIdx)
    {
        elemIdx = localIdx * SMALL_SORT_ELEMENTS_PER_THREAD + loopIdx;
        if (elemIdx < (uint)ElementCount)
        {
            SmallSortKeys[elemIdx] = KeysIn[elemIdx];
            SmallSortValues[elemIdx] = ValuesIn[elemIdx];
        }
        else
        {
            SmallSortKeys[elemIdx] = 0xFFFFFFFF;  // Max value for padding
            SmallSortValues[elemIdx] = 0;
        }
    }
    GroupMemoryBarrierWithGroupSync();

    // Perform 3 passes of 8-bit radix sort in shared memory (24-bit keys)
    for (int pass = 0; pass < 3; ++pass)
    {
        int bitOff = pass * 8;

        //=====================================================================
        // Step 1: Clear histogram
        //=====================================================================
        if (localIdx < RADIX_SIZE)
        {
            SmallHistogram[localIdx] = 0;
        }
        GroupMemoryBarrierWithGroupSync();

        //=====================================================================
        // Step 2: Build histogram (only count valid elements)
        //=====================================================================
        [unroll]
        for (loopIdx = 0; loopIdx < SMALL_SORT_ELEMENTS_PER_THREAD; ++loopIdx)
        {
            elemIdx = localIdx * SMALL_SORT_ELEMENTS_PER_THREAD + loopIdx;
            if (elemIdx < (uint)ElementCount)
            {
                uint digit = (SmallSortKeys[elemIdx] >> bitOff) & RADIX_MASK;
                InterlockedAdd(SmallHistogram[digit], 1);
            }
        }
        GroupMemoryBarrierWithGroupSync();

        //=====================================================================
        // Step 3: Exclusive prefix sum on histogram (256 buckets)
        // Wave path: O(log n) parallel using WavePrefixSum + LDS
        // Fallback: O(n) sequential single-thread
        //=====================================================================
#if COMPILER_SUPPORTS_WAVE_INTRINSICS
        {
            // All 256 threads participate in parallel prefix sum
            uint laneIdx = WaveGetLaneIndex();
            uint waveSize = WaveGetLaneCount();
            uint waveIdx = localIdx / waveSize;

            uint myCount = SmallHistogram[localIdx];
            uint wavePrefixSum = WavePrefixSum(myCount);
            uint waveTotal = WaveActiveSum(myCount);

            if (WaveIsFirstLane())
            {
                SmallWaveTotals[waveIdx] = waveTotal;
            }
            GroupMemoryBarrierWithGroupSync();

            uint crossWavePrefix = 0;
            for (uint w = 0; w < waveIdx; ++w)
            {
                crossWavePrefix += SmallWaveTotals[w];
            }

            SmallPrefixSum[localIdx] = crossWavePrefix + wavePrefixSum;
        }
        GroupMemoryBarrierWithGroupSync();
#else
        if (localIdx == 0)
        {
            uint sum = 0;
            for (uint b = 0; b < RADIX_SIZE; ++b)
            {
                uint count = SmallHistogram[b];
                SmallPrefixSum[b] = sum;
                sum += count;
            }
        }
        GroupMemoryBarrierWithGroupSync();
#endif

        //=====================================================================
        // Step 4: Scatter to temp buffer using atomic increment
        //=====================================================================
        [unroll]
        for (loopIdx = 0; loopIdx < SMALL_SORT_ELEMENTS_PER_THREAD; ++loopIdx)
        {
            elemIdx = localIdx * SMALL_SORT_ELEMENTS_PER_THREAD + loopIdx;
            if (elemIdx < (uint)ElementCount)
            {
                uint key = SmallSortKeys[elemIdx];
                uint value = SmallSortValues[elemIdx];
                uint digit = (key >> bitOff) & RADIX_MASK;

                uint outPos;
                InterlockedAdd(SmallPrefixSum[digit], 1, outPos);

                SmallSortTemp[outPos] = key;
                SmallSortTempV[outPos] = value;
            }
        }
        GroupMemoryBarrierWithGroupSync();

        //=====================================================================
        // Step 5: Copy back from temp to main arrays
        //=====================================================================
        [unroll]
        for (loopIdx = 0; loopIdx < SMALL_SORT_ELEMENTS_PER_THREAD; ++loopIdx)
        {
            elemIdx = localIdx * SMALL_SORT_ELEMENTS_PER_THREAD + loopIdx;
            SmallSortKeys[elemIdx] = SmallSortTemp[elemIdx];
            SmallSortValues[elemIdx] = SmallSortTempV[elemIdx];
        }
        GroupMemoryBarrierWithGroupSync();
    }

    // Write results to output
    [unroll]
    for (loopIdx = 0; loopIdx < SMALL_SORT_ELEMENTS_PER_THREAD; ++loopIdx)
    {
        elemIdx = localIdx * SMALL_SORT_ELEMENTS_PER_THREAD + loopIdx;
        if (elemIdx < (uint)ElementCount)
        {
            KeysOut[elemIdx] = SmallSortKeys[elemIdx];
            ValuesOut[elemIdx] = SmallSortValues[elemIdx];
        }
    }
}

//=============================================================================
// ParticleID Sorting - Direct read from Particles buffer
// Used for sorting particles by ParticleID before CPU readback.
// Lower ParticleID = older particle (spawned earlier).
//=============================================================================

#include "FluidGPUPhysics.ush"

StructuredBuffer<FGPUFluidParticle> Particles;

// Histogram pass - reads ParticleID directly from Particles buffer
[numthreads(THREAD_GROUP_SIZE, 1, 1)]
void RadixSortHistogramParticleIDCS(
    uint3 GroupId : SV_GroupID,
    uint3 GroupThreadId : SV_GroupThreadID,
    uint3 DispatchThreadId : SV_DispatchThreadID)
{
    uint localIdx = GroupThreadId.x;
    uint groupIdx = GroupId.x;
    uint globalBase = groupIdx * ELEMENTS_PER_GROUP;

    // Initialize local histogram (256 threads, 256 buckets - 1:1)
    LocalHistogram[localIdx] = 0;
    GroupMemoryBarrierWithGroupSync();

    // Load elements and count - read ParticleID directly from Particles
    [unroll]
    for (uint i = 0; i < ELEMENTS_PER_THREAD; ++i)
    {
        uint elementIdx = globalBase + localIdx * ELEMENTS_PER_THREAD + i;

        if (elementIdx < (uint)ElementCount)
        {
            uint key = (uint)Particles[elementIdx].ParticleID;
            uint digit = (key >> BitOffset) & RADIX_MASK;
            InterlockedAdd(LocalHistogram[digit], 1);
        }
    }
    GroupMemoryBarrierWithGroupSync();

    // Write local histogram to global histogram
    Histogram[groupIdx * RADIX_SIZE + localIdx] = LocalHistogram[localIdx];
}

// Scatter pass - reads ParticleID directly from Particles buffer
[numthreads(THREAD_GROUP_SIZE, 1, 1)]
void RadixSortScatterParticleIDCS(
    uint3 GroupId : SV_GroupID,
    uint3 GroupThreadId : SV_GroupThreadID)
{
    uint localIdx = GroupThreadId.x;
    uint groupIdx = GroupId.x;
    uint globalBase = groupIdx * ELEMENTS_PER_GROUP;
    uint warpId = localIdx / WARP_SIZE;

    // Thread-local storage
    uint myKeys[ELEMENTS_PER_THREAD];
    uint myValues[ELEMENTS_PER_THREAD];
    uint myDigits[ELEMENTS_PER_THREAD];
    uint myValid[ELEMENTS_PER_THREAD];

    // Load elements - read ParticleID from Particles
    [unroll]
    for (uint i = 0; i < ELEMENTS_PER_THREAD; ++i)
    {
        uint elementIdx = globalBase + localIdx * ELEMENTS_PER_THREAD + i;
        uint localElementIdx = localIdx * ELEMENTS_PER_THREAD + i;

        if (elementIdx < (uint)ElementCount)
        {
            myKeys[i] = (uint)Particles[elementIdx].ParticleID;
            myValues[i] = elementIdx;  // Index as value for reordering
            myDigits[i] = (myKeys[i] >> BitOffset) & RADIX_MASK;
            myValid[i] = 1;
            ElementDigits[localElementIdx] = myDigits[i];
        }
        else
        {
            myKeys[i] = 0xFFFFFFFF;
            myValues[i] = 0;
            myDigits[i] = 0xFF;
            myValid[i] = 0;
            ElementDigits[localElementIdx] = 0xFFFFFFFF;
        }
    }

    // Clear and build warp histograms
    for (uint b = localIdx; b < NUM_WARPS * RADIX_SIZE; b += THREAD_GROUP_SIZE)
    {
        WarpBucketData[b] = 0;
    }
    GroupMemoryBarrierWithGroupSync();

    [unroll]
    for (uint i = 0; i < ELEMENTS_PER_THREAD; ++i)
    {
        if (myValid[i])
        {
            InterlockedAdd(WarpBucketData[warpId * RADIX_SIZE + myDigits[i]], 1);
        }
    }
    GroupMemoryBarrierWithGroupSync();

    // Compute prefix sums
    {
        uint bucket = localIdx;
        DigitBaseOffsets[bucket] = GlobalOffsetsSRV[bucket] + HistogramSRV[groupIdx * RADIX_SIZE + bucket];

        uint sum = 0;
        for (uint w = 0; w < NUM_WARPS; ++w)
        {
            uint idx = w * RADIX_SIZE + bucket;
            uint count = WarpBucketData[idx];
            WarpBucketData[idx] = sum;
            sum += count;
        }
    }
    GroupMemoryBarrierWithGroupSync();

    // Scatter with stable intra-warp ranking
    uint warpStartElement = warpId * (WARP_SIZE * ELEMENTS_PER_THREAD);

    [unroll]
    for (uint j = 0; j < ELEMENTS_PER_THREAD; ++j)
    {
        if (myValid[j])
        {
            uint digit = myDigits[j];
            uint myElementIdx = localIdx * ELEMENTS_PER_THREAD + j;
            uint myWarpLocalIdx = myElementIdx - warpStartElement;

            uint rankInWarp = 0;
            for (uint k = 0; k < myWarpLocalIdx; ++k)
            {
                if (ElementDigits[warpStartElement + k] == digit)
                {
                    rankInWarp++;
                }
            }

            uint globalPos = DigitBaseOffsets[digit]
                           + WarpBucketData[warpId * RADIX_SIZE + digit]
                           + rankInWarp;

            KeysOut[globalPos] = myKeys[j];
            ValuesOut[globalPos] = myValues[j];
        }
    }
}
